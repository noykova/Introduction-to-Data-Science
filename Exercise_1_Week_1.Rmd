---
title: "Exercise_1_Week_1"
output: html_document
---
# Exercise set 1

# Exercise 1: preprocessing and general handling of the data.

##1.1. Reading the data 

Here we use part of the Titanic data described [here](https://www.kaggle.com/c/titanic/data). The data include information about the passangers on the board of the RMS Titanic. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. 


```{r}
titanic <- read.csv(file="train.csv",header=TRUE,sep=",",na.strings=c(""," ","NA"))

```

##1.2.	Have a look at the data. 

Look at the dimension, structure and column names of the data:

```{r}
library(dplyr)
dim(titanic)
str(titanic)
colnames(titanic)
summary(titanic)

```

The original data involve 891 observations of 12 variables. 

We will build new representations of the dataset that are better suited for a particular purpose. Some of the columns, e.g Name, simply identify a person and cannot be useful for prediction tasks - remove them.

I decided to delete the following columns: *Name*, *sibsp* (# of siblings / spouses aboard the Titanic, and *parch* (# of parents / children aboard the Titanic). 

```{r}
keep <- c("PassengerId", "Survived", "Pclass", "Sex", "Age", "Ticket", "Fare", "Cabin")
tit <- select(titanic, one_of(keep))
glimpse(tit)

```



#1.3.	Take only the first character feom column Cabin and add it as a new column to the table.

```{r}
tit$Cab <- substr(tit$Cabin, 0, 1)
glimpse(tit)

```



#1.4.	Transform string to categorical column values. 

You'll notice that some of the columns, such as the previously added deck number are categorical. Their representation as a string is not efficient for further computation. Transform them into numeric values so that a unique integer id corresponds to each distinct category. Hint. pandas can do this for you.

```{r}
tit$Cab <- as.factor(tit$Cab)
glimpse(tit)

```

#1.5.	Imputation of the missing data

Some of the rows in the data have missing values, e.g when the cabin number of a person is not known. Most machine learning algorithms have trouble with missing values, and they need to be handled in preprocessing:

#a) For continous values, replace the missing values with the average of the non-missing values of that column.

The following ifelse() function returns the imputed mean value if its first argument is NA. Otherwise, it returns the first argument: 

```{r}
tit$Age <- ifelse(is.na(tit$Age), mean(tit$Age, na.rm=TRUE), tit$Age)
glimpse(tit)

```


#b) For discrete and categorical values, replace the missing values with the mode of the column.

```{r}
library(Hmisc)
tit$Cabin <- impute(tit$Cabin, mean)
tit$Cab <- impute(tit$Cab, mean)
glimpse(tit)

```


#1.6.	Exporting the data as .csv and .json formats.

# Exprort the data as .csv file

```{r}
write.csv(tit, file = "titCsv.csv")

```

# Exprort the data as .json file

```{r}
library(rjson)
titJson <- toJSON(unname(split(tit, 1:nrow(tit))))
cat(titJson)

```

The created .json data format is different than the structure required in the exercises set: 

[
    {
        "Deck": 0,
        "Age": 20,
        "Survived", 0
        ...
    },
    {
        ...
    }
]


**Conclusion:** R is niot the best language for creating, editing and saving .json files. 
Therefore I continue the work in Python. 
The code for producing .json file is: 

```
import csv
import json

#convert .csv to .json in Python. 
  
# Open the CSV  
f = open( 'titCsv.csv', 'rU' )  
# Change each fieldname to the appropriate field name. I know, so difficult.  
reader = csv.DictReader( f, fieldnames = ( "PasengerID","Survived","Pclass","Sex","Age","Ticket","Fare","Cabin","Cab" ))  
# Parse the CSV into JSON  
out = json.dumps( [ row for row in reader ] )  
print ("JSON parsed!")  
# Save the JSON  
f = open( 'parsed.json', 'w')  
f.write(out)  
print ("JSON saved!")  


```

The output file parsed.json is imported and checked in R. 

```{r}
result <- fromJSON(file = "parsed.json")

```

The result is worse than the previous one, obtained in R.

# Exercise 2: Text data.

We'll are looking into Amazon reviews, and the steps needed to transform a raw dataset into one more suitable for prediction tasks.

#2.1	Download the data in .json format.

```{r}
result <- fromJSON(file = "Automotive_5.json")
print(result)
```
It seems that the reading did not succeed, and only the first json object was read. 

Useful library for dealing with .json objects . The tutorial is given [here](https://cran.r-project.org/web/packages/tidyjson/vignettes/introduction-to-tidyjson.html)
eBook about handling and processing strings in R [here](http://gastonsanchez.com/Handling_and_Processing_Strings_in_R.pdf)

#2.2 Downcase the contents, remove all punctuation and stop-words in the reviewText field
```{r}
library(tm)
textBeg<-result[['reviewText']]
textBeg <- result$reviewText
textBeg = tolower(textBeg) #make it lower case
textBeg
textBeg = gsub('[[:punct:]]', '', textBeg) #remove punctuation
textBeg
#remove stop-words
stopwords_regex = paste(stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
textBeg = stringr::str_replace_all(textBeg, stopwords_regex, '')
textBeg
```


d) Apply a stemmer on the paragraphs, so that inflected forms are mapped to the base form. For example, for python the popular natural language toolkit nltk has an easy-to-use stemmer.
e) Filter the data by selecting reviews where the field overall is 4 or 5, and store the review texts in file pos.txt. Similarly, select reviews with rating 1 or 2 and store the reviews in file neg.txt. (Ignore the reviews with overall rating 3.) Each line in the two files should contain exactly one preprocessed review text without the rating.
Having created two collections of positive and negative reviews, respectively, you may wish to take a quick look to see how the review texts differ between them. Here too, we will be using this data later to experiment with machine learning methods.


# Exercise 3: SQL data (Baseball).

##3.1. Reading the data 

This exercise involves an SQL database with historical data on baseball players.First I download the sqlite version of the dataset from Kaggle and unzip in sepatrate data directory.
For further analysis I have used RSQLite and DBI packages in R. 
Connection with the database and list of the tables involved is provided as: 

```{r}
library("RSQLite")
library(DBI)
drv <- dbDriver("SQLite")
con <- dbConnect(drv, dbname = "database.sqlite")
dbListTables(con)
dbListFields(con, "player")
```


#3.2	Join the tables player and hall_of_fame by the key player_id which appears in both. Select the rows where the field inducted takes value 'Y' (i.e., those entries where the player in question was voted to the Hall of Fame). 
We get 312 famous players, including Babe Ruth and Yogi Berra.

```{r}
joinedresults <- dbSendQuery(con, "SELECT * FROM 
player INNER JOIN hall_of_fame USING(player_id) WHERE inducted = 'Y'")
fetch(joinedresults)
dbDisconnect(con)
```

#3.3.	Join in a third table, player_college, again by the key player_id. The player_college table indicates for which college (university) the player has played, if any, in each year.

The number of rows in the table player_college per player varies between zero and nine.

We have to Keep the restriction that the player should have been selected to the Hall of Fame (inducted = 'Y'), and group the output by college (college_id). 

```{r}
drv <- dbDriver("SQLite")
con <- dbConnect(drv, dbname = "database.sqlite")
#list the tables in the database
dbListTables(con)

joined <-dbSendQuery(con, "SELECT DISTINCT * from (SELECT * FROM 
player INNER JOIN hall_of_fame USING(player_id) WHERE inducted = 'Y') INNER JOIN player_college USING(player_id) WHERE inducted = 'Y' GROUP BY college_id")
fetch(joined)
dbDisconnect(con)

```


Next we have to count how many Hall of Fame players are alumni of each college. There is one college that has as many as three. Which one? (To see the full name of the college, you can look up the college_id in table college). This is better to be provided in Python. 









